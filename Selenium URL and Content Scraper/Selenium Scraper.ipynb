{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium Scraper Using a Headless Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sjifra de Leeuw**   \n",
    "*Amsterdam School of Communication Research, University of Amsterdam*   \n",
    "*Social Media and Political Participation Lab, New York University* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is web scraping?\n",
    "\n",
    "Web scraping or \"web harvesting\" is another word for extracting data from websites. In this example, we will scrape the publication history of the New York times. We do so by first artificially creating a list of urls, each of which will open the history page containing all articles published on that particular day. Within each of these days, we collect the html elements we're interested in for each separate article, i.e. the short description as well as the url to access the full article. \n",
    "\n",
    "\n",
    "## What is a headless browser and why do I need it?\n",
    "\n",
    "When scraping websites, a browser will open that is controlled programatically via the code you have written. Unless specified otherwise, this means that the graphical user interface will appear on your screen. This puts an unnecessary claim on your CPU and memory usage. This can be avoided by using a headless browser, which is a browser without a graphical user interface. Google Chrome has announced that they will enable a headless browser in one of the following updates. However, a headless browser is already available in the Chrome Canary channel. Instructions on how to install a headless driver can be found here: https://duo.com/decipher/driving-headless-chrome-with-python\n",
    "\n",
    "## What do I need? \n",
    "\n",
    "- **Chrome Canary:** https://www.google.com/chrome/canary/\n",
    "- **Chrome Driver:** https://chromedriver.chromium.org/\n",
    "- **Selenium:** https://pypi.org/project/selenium/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following **libraries**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium libraries\n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from selenium.webdriver.chrome.options import Options \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException \n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "from selenium import webdriver \n",
    "\n",
    "# Other libraries\n",
    "import os  \n",
    "import time \n",
    "import datetime \n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the **search queries**, create a vector of dates. Then use these dates to create a vector of urls, each of which will open a search query for a different date. Do pay special attention to the date-format in the url. In our case, dates are fomatted as YYYYMMDD, without any hyphens. Dates may be formatted differently on other websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime.strptime(\"01-01-1992\", \"%d-%m-%Y\") # start date query\n",
    "end = datetime.datetime.strptime(\"01-01-2019\", \"%d-%m-%Y\") # end date query\n",
    "date_vector = [start + datetime.timedelta(days=x) for x in range(0, (end-start).days)] # dates as numeric vector\n",
    "date_vector_chr = np.array([date_obj.strftime('%Y%m%d') for date_obj in date_vector]) # dates as character vector\n",
    "df = pd.DataFrame(data = {'date': date_vector, 'date_chr': date_vector_chr}) # df containing both vectors\n",
    "df[\"url\"] = \"https://www.nytimes.com/search?dropmab=false&endDate=\" + df.date_chr + \"&query=&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=best&startDate=\" + df.date_chr + \"&types=article\"\n",
    "urls = np.array(df.url) # isolated vector of urls\n",
    "dates = np.array(df.date_chr) # isolated vector of dates as characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an **empty dataframe**. The new data collected in the scraping process will be appended to this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = pd.DataFrame(np.empty((0, 3), dtype = np.str))\n",
    "links = links.rename(columns={0: \"url\", 1: \"text\", 2: \"date\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scroll_page(url)` is a function that scrolls down to the bottom of the page, by pressing the \"show more button\" by\n",
    "\n",
    "- checking whether a button is present (using a try-except statement)\n",
    "- if present, clicks the button\n",
    "- then do nothing for a randomized number of seconds (to avoid being intercepted by the server)\n",
    "- stop unfolding the page when the button is no longer present (using a try-except statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_page(url):\n",
    "    #button_element = browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[2]/div/button')\n",
    "    try:\n",
    "        button_element = browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[2]/div/button')\n",
    "    except:\n",
    "        button_element = None\n",
    "    while button_element: # is present\n",
    "        button_element.click() # click button\n",
    "        time.sleep(np.random.uniform(2, 6)) # wait randomized number of seconds\n",
    "        try:\n",
    "            button_element = browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[2]/div/button')\n",
    "        except:\n",
    "            button_element = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collect(old_dataframe)` is a function to identify separate articles, collect urls and text and append to the dataframe produced in a prior iteration by:\n",
    "\n",
    "- looping over a vector of elements (which we called `link_element` in the code below) containing links. \n",
    "- within these elements, it finds all attributes indicating that a url is to follow, i.e. all attributes starting with `href`\n",
    "- within the link elements it then also saves the texts `link_text`\n",
    "- if `link_text` indicates that the article is part of a `PRINT EDITION` it \n",
    "  - collects all urls in `link_element` an array called `url_array`\n",
    "  - collects all text in `link_element` in an array called `text_array` \n",
    "  - binds two columns into dataframe called `newdata`\n",
    "  - adds a new column containing the date of publication by using the command `newdata[\"date\"]\n",
    "  - now that the newdata have the same dimensions as the old data, appends the newdata to the old data using `old_dataframe.append(newdata)`\n",
    "  - `return` the old dataframe with the new data included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect(old_dataframe):\n",
    "    for j in range(0, len(link_element)): # for each element in the vector of link elements\n",
    "        link = link_element[j] # select one element in vector\n",
    "        url = link.get_attribute(\"href\") # get the url connected to the element\n",
    "        link_text = link.text # get the text connected to the element\n",
    "        if \"PRINT EDITION\" in link_text: # if text in element contains \"PRINT EDITION\"\n",
    "            url_array = np.array(url) # collect urls in array\n",
    "            text_array = np.array(link_text) # collect text in array \n",
    "            newdata = pd.DataFrame(data = {'url': url_array, 'text': text_array}, index=[0]) # bind arrays in df\n",
    "            newdata[\"date\"] = dates[i] # add new column containing the date\n",
    "            old_dataframe = old_dataframe.append(newdata) # append to already existing df\n",
    "    return(old_dataframe) # return the existing df including the new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scraper combines these pieces and:\n",
    "\n",
    "- for each url `i` in vector of urls it \n",
    "- opens a headless chrome browser (change directories prior to running) \n",
    "- lists the relevant url `url = urls[i]`\n",
    "- gets the url via `browser.get(url)`\n",
    "- tests if \"show more button\" is present via a `try-except` statement\n",
    "- if present it unfolds the page using the `scroll_page()` function\n",
    "- creates a vector of link elements \n",
    "- for each of these link elements it collects the url, text and date via the `collect()` function\n",
    "- then the browser quits `browser.quit()` and moves to the next iteration `i`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Starting Time: \" + str(datetime.datetime.now()))\n",
    "for i in range(0, len(urls)):\n",
    "    try: \n",
    "        chrome_options = Options()  \n",
    "        chrome_options.add_argument(\"--headless\")  \n",
    "        chrome_options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'  \n",
    "        browser = webdriver.Chrome(executable_path='path/chromedriver', options=chrome_options) \n",
    "        url = urls[i]\n",
    "        browser.get(url) \n",
    "        try:\n",
    "            button_element = browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[2]/div/button')\n",
    "        except:\n",
    "            button_element = None\n",
    "        scroll_page(url)\n",
    "        link_element = browser.find_elements_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[1]/ol/li[.]/div/div/a')\n",
    "        links = collect(links)\n",
    "        browser.quit()\n",
    "        links.to_csv(\"nyt_urls_1991_2018.csv\", index=False)\n",
    "        print(str(i) + \" out of \" + str(len(urls)), end=\"\\r\")\n",
    "    except:\n",
    "        pass \n",
    "print (\"End Time: \" + str(datetime.datetime.now()))\n",
    "os.system(\"I am done collecting urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now ignored all errors, by writing `except: pass`. This ensured that our scraper continues collecting even when encountering an error. We can now obtain the days for which it has returned an error and collect them separately (if necessary) by matching the days in our date-vector with the days in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import new dataset\n",
    "df = pd.read_csv('path/nyt_complete.csv') # edit path \n",
    "df['date'] = pd.to_numeric(df.date, downcast='signed')\n",
    "# Convert date-vector to numeric\n",
    "date_vector_numeric = pd.to_numeric(date_vector_chr, downcast='signed')\n",
    "# Create an empty missing dataframe\n",
    "missing = pd.DataFrame(np.empty((0, 1)))\n",
    "missing = missing.rename(columns={0: \"date_miss\"})\n",
    "# Check if value in vector is present in data frame  \n",
    "for i in range(0, len(date_vector_numeric)): \n",
    "    date = date_vector_numeric[i]\n",
    "    if any(df.date == date) == False: # if not append to missing dataframe\n",
    "        print(str(i + 1) + \" out of \" + str(len(date_vector_numeric)), end=\"\\r\")\n",
    "        date_mis = pd.DataFrame({'date_miss': date}, index=[0]) \n",
    "        missing = missing.append(date_mis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the relevant urls for the missing dates and scrape the remaining pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing.date_miss = pd.to_numeric(missing.date_miss, downcast='signed') # convert float to integer\n",
    "missing.date_miss = missing.date_miss.astype(str)\n",
    "missing['url'] = \"https://www.nytimes.com/search?dropmab=false&endDate=\" + missing.date_miss + \"&query=&sections=U.S.%7Cnyt%3A%2F%2Fsection%2Fa34d3d6c-c77f-5931-b951-241b4e28681c&sort=best&startDate=\" + missing.date_miss + \"&types=article\"\n",
    "urls = np.array(missing.url) # isolated vector of urls\n",
    "dates = np.array(missing.date_miss) # isolated vector of dates as characters\n",
    "\n",
    "# Create empty dataframe\n",
    "links = pd.DataFrame(np.empty((0, 3), dtype = np.str))\n",
    "links = links.rename(columns={0: \"url\", 1: \"text\", 2: \"date\"})\n",
    "\n",
    "# Scraper\n",
    "print (\"Starting Time: \" + str(datetime.datetime.now()))\n",
    "for i in range(0, len(urls)):\n",
    "    try: \n",
    "        chrome_options = Options()  \n",
    "        chrome_options.add_argument(\"--headless\")  \n",
    "        chrome_options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'  \n",
    "        browser = webdriver.Chrome(executable_path='path/chromedriver', options=chrome_options) \n",
    "        url = urls[i]\n",
    "        browser.get(url) \n",
    "        try:\n",
    "            button_element = browser.find_element_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[2]/div/button')\n",
    "        except:\n",
    "            button_element = None\n",
    "        scroll_page(url)\n",
    "        link_element = browser.find_elements_by_xpath('//*[@id=\"site-content\"]/div/div[2]/div[1]/ol/li[.]/div/div/a')\n",
    "        links = collect(links)\n",
    "        browser.quit()\n",
    "        links.to_csv(\"url_collection_missing.csv\", index=False)\n",
    "        print(str(i) + \" out of \" + str(len(urls)), end=\"\\r\")\n",
    "    except:\n",
    "        pass \n",
    "print (\"End Time: \" + str(datetime.datetime.now()))\n",
    "os.system(\"I am done collecting urls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append to arleady collected urls and save as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('path/nyt_urls_1991_2018.csv')\n",
    "df2 = pd.read_csv('path/url_collection_missing.csv')\n",
    "df = df1.append(df2)\n",
    "df.to_csv(\"urls_complete.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article Scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have obtained a dataset of URLS, we can now start scraping the content of each webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium libraries\n",
    "from selenium.webdriver.common.keys import Keys  \n",
    "from selenium.webdriver.chrome.options import Options \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC \n",
    "from selenium.common.exceptions import TimeoutException \n",
    "from selenium.common.exceptions import NoSuchElementException \n",
    "from selenium import webdriver \n",
    "\n",
    "# Other libraries\n",
    "import os  \n",
    "import time \n",
    "import datetime \n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataframe\n",
    "df = pd.read_csv('path/urls_complete.csv')\n",
    "\n",
    "# Separate columns into arrays\n",
    "urls = np.array(df.url) \n",
    "dates = np.array(df.date) \n",
    "overview = np.array(df.text) \n",
    "\n",
    "# Create empty dataframe\n",
    "content_df = pd.DataFrame(np.empty((0, 4), dtype = np.str)) #\n",
    "content_df = content_df.rename(columns={0: \"url\", 1: \"text\", 2: \"date\", 3: \"content\"})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over urls and combine all info in dataframe\n",
    "for i in range(0, ):\n",
    "    # obtain values from arrays\n",
    "    url = urls[i]\n",
    "    date = dates[i]\n",
    "    text = overview[i]\n",
    "    # open url \n",
    "    chrome_options = Options()  \n",
    "    chrome_options.add_argument(\"--headless\")  \n",
    "    chrome_options.binary_location = '/Applications/Google Chrome Canary.app/Contents/MacOS/Google Chrome Canary'  \n",
    "    browser = webdriver.Chrome(executable_path='path/chromedriver', options=chrome_options) \n",
    "    browser.get(url) \n",
    "    # collect text \n",
    "    content_element = browser.find_element_by_xpath('//*[@id=\"story\"]')\n",
    "    if content_element:\n",
    "        content = content_element.text # get the text connected to the element\n",
    "    else: \n",
    "        content = \"NA\"\n",
    "    newdata = pd.DataFrame(data = {'url': url, 'text': text, 'date': date, 'content': content}, index=[0]) # bind arrays in df  \n",
    "    content_df = content_df.append(newdata) # append to already existing df\n",
    "    content_df.to_csv(\"nyt-urls-with-content-1992-2018.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
